<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[用ADO导入CSV后部分数据变成NULL，怎么破？]]></title>
    <url>%2F2018%2F06%2F20%2FADO-Text-Driver-Data-Type%2F</url>
    <content type="text"><![CDATA[​ 最近要处理一些数据量比较大的CSV文件，本着能不用ACCESS就不用ACCESS的原则，使用了VBA+ADO联用的方法。 CSV，Comma Separated Value，是一种用逗号分隔不同的值，来保存数据的文本文件 ADO, Active Data Object，可以简单的看作是操作数据库的接口 出现了什么问题？首先要做的一步是把CSV中的数据读取到工作表里。鉴于这不是一篇入门的文章，我们就不在这里介绍基础内容了。 现有的p.csv文件是这个样子： 用下面的这段程序，可以直接把CSV中的数据读入到工作表里。 12345678910111213141516171819Option ExplicitSub main() LoadData "p.csv"End SubPrivate Sub LoadData(strCSVPath As String) Dim strSQL As String, strConnection As String Dim oRst As ADODB.Recordset strConnection = "Provider=Microsoft.ace.oledb.12.0;" &amp; _ "Data Source=" &amp; ThisWorkbook.Path &amp; ";" &amp; _ "Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';" Set oRst = New ADODB.Recordset strSQL = "SELECT * FROM " &amp; strCSVPath oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly Sheet1.Range("a1").CopyFromRecordset oRst oRst.Close Set oRst = NothingEnd Sub 上面这段代码，可以把当前工作簿目录下的p.csv中的数据内容导入到sheet1工作表里。查看sheet1工作表，发现一个问题：有些单元格是空的，比如第189，195,196行。 再返回到源文件查看对应的行，第一眼看上去是有数据的啊，见下图： 又重复运行了几次脚本，复制出来的数据始终有空行，百思不得其解。 原因在哪里？又盯着看了一会，似乎看出了点端倪。不能被显示出来的数据貌似都很大，都是E9数量级。但看第188行，1.22E9是可以正常显示的。 感觉显示为空单元的数字都太大了，所以没有显示出来。经过LEAF大神的提醒，可以使用Fields.Type来查看数据类型。上面的代码修改为： 1234567891011121314151617181920212223242526Option ExplicitSub main() LoadData "p.csv"End SubPrivate Sub LoadData(strCSVPath As String) Dim strSQL As String, strConnection As String Dim oRst As ADODB.Recordset Dim i As Integer strConnection = "Provider=Microsoft.ace.oledb.12.0;" &amp; _ "Data Source=" &amp; ThisWorkbook.Path &amp; ";" &amp; _ "Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';" Set oRst = New ADODB.Recordset strSQL = "SELECT * FROM " &amp; strCSVPath oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly '以下为新增代码 '======== For i = 0 To oRst.Fields.Count - 1 Debug.Print oRst.Fields(i).Type, oRst.Fields(i).Name Next i '======== '以上为新增代码 Sheet1.Range("a1").CopyFromRecordset oRst oRst.Close Set oRst = NothingEnd Sub 到立即窗口查看，打印出了下面的内容：123202 账户简称3 金额（元）7 结算日期 也就是说，金额字段对应的数据类型是3。数据类型3又是什么呢？直接在msdn上搜索一下，发现这个页面里说的很清楚： 3对应的是4个字节的正负整数。4个字节大约可以表示从-2^31到2^31之间，大约是-2.15E9到2.15E9。难怪4.7E9显示不出来，因为变量长度溢出了啊。 为什么会出现这个问题？在用ADO读取文本文件的时候，因为文本文件中的数据格式没有被显式指定，所以ADO的文本引擎会先扫描一定的行数，然后给每列推测一个数据类型。 很显然，文中出现的问题就是因为ADO扫描完之后，指定了一个adInteger类型，但后面出现的大的数字已经超出了范围。 那么，ADO会扫描多少行呢？在网上搜了一下，在这个注册表下面有ADO文本引擎的默认设置：[Computer\HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\Jet\4.0\Engines\Text] 很明显，MaxScanRows的值是25，也就是说Text引擎会扫描25行，然后根据扫描结果指定一个数据类型。 怎么解决？有两种方法。 第一种方法不太推荐，就是直接修改注册表，把MaxScanRows的值修改为足够大。如果想扫描全部内容，就把MaxScanRows的值改为0。 第二种方法更加安全一些，也就是用一个文件Schema.ini来存储文本文件的信息。 Schema.ini一定要与CSV文件在同一个目录下，他包含了下面这些信息： 文件名 文件格式 字段名 字符集 等等 详细的格式设置可以查看这个链接 在这个例子里，我们就在p.csv所在的文件夹创建Schema.ini，然后简单的指定下面的内容即可。 1234[p.csv]Format = CSVDelimitedColNameHeader = TrueMaxScanRows = 0 当然，我们也可以把创建文件的内容加入到ADO脚本里，最后完整的脚本内容就会变成： 12345678910111213141516171819202122232425262728293031323334353637Option ExplicitSub main() CreateIniFile "p.csv" LoadData "p.csv"End SubPrivate Sub LoadData(strCSVPath As String) Dim strSQL As String, strConnection As String Dim oRst As ADODB.Recordset Dim i As Integer strConnection = "Provider=Microsoft.ace.oledb.12.0;" &amp; _ "Data Source=" &amp; ThisWorkbook.Path &amp; ";" &amp; _ "Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';" Set oRst = New ADODB.Recordset strSQL = "SELECT * FROM " &amp; strCSVPath oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly Sheet1.Range("a1").CopyFromRecordset oRst oRst.Close Set oRst = NothingEnd SubPrivate Sub CreateIniFile(strCSVPath As String) Dim iFreeFile As Integer Dim StrIni As String StrIni = "[" &amp; strCSVPath &amp; "]" &amp; vbCrLf &amp; _ "Format = CSVDelimited" &amp; vbCrLf &amp; _ "ColNameHeader = True" &amp; vbCrLf &amp; _ "MaxScanRows = 0" iFreeFile = FreeFile Open ThisWorkbook.Path &amp; "\Schema.ini" For Output As #iFreeFile Print #iFreeFile, StrIni Close #iFreeFileEnd Sub 再次运行程序，发现之前的空白单元格已经有数字进来了： 总结 VBA还是要多查MSDN ADO里的Field的基本属性要熟练]]></content>
      <categories>
        <category>VBA</category>
      </categories>
      <tags>
        <tag>ADO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用scrapy爬取职位信息]]></title>
    <url>%2F2018%2F06%2F15%2F%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[​ 新开一篇，来讲解一个51JOB工作职位的爬取。目标：爬取51JOB上所有与化工相关的职位信息。 工具：scrapy框架+Python3 第一步：安装环境使用： 1virtualenv -p /usr/bin/python3 scrapy_env 来创建一个新的虚拟环境来运行scrapy cd到scrapy_env文件夹，然后执行下面语句来开启虚拟空间： 1~/scrapy_env$ source bin/activate 进入到scrapy_env的虚拟空间后，用pip3 install scrapy来安装scrapy所需要的依赖 创建Scrapy项目使用scrapy startproject来创建项目。 12345678910$ scrapy startproject 51jobError: Project names must begin with a letter and contain onlyletters, numbers and underscores~$ scrapy startproject jobsNew Scrapy project 'jobs', using template directory '/home/ubuntu/scrapy_env/lib/python3.5/site-packages/scrapy/templates/project', created in: /home/ubuntu/jobsYou can start your first spider with: cd jobs scrapy genspider example example.com 然后使用scrapy genspider来创建一个爬虫 123~/jobs$ scrapy genspider JobSpider search.51job.comCreated spider 'JobSpider' using template 'basic' in module: jobs.spiders.JobSpider 这样，项目的框架就搭好了。现在整个项目的目录文件应该是这样子的： 123456789101112131415.├── jobs│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── __pycache__│ │ ├── __init__.cpython-35.pyc│ │ └── settings.cpython-35.pyc│ ├── settings.py│ └── spiders│ ├── __init__.py│ ├── JobSpider.py│ └── __pycache__└── scrapy.cfg 编辑Scrapy文件Scrapy框架极大程度的减少了我们爬虫额代码编写量。对于一个简单的爬虫，我们只需要修改下面几个内容。 首次得打开网页链接：1https://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare= items.py我们需要获取的每一条招聘内容包括了下面这些信息： 职位名 公司名 工作地点 薪资 发布时间因此，我们需要根据这些来定义一个Item，也就是一条爬取下来的信息。 1234567891011import scrapyclass JobsItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() job_title = scrapy.Field() company = scrapy.Field() job_href = scrapy.Field() location = scrapy.Field() salary = scrapy.Field() post_date = scrapy.Field() 这就好了，讲白了，我们是定义了一个名字叫JobItem的字典，然后给这个字典定义了5个键。 spiders/JobSpider.py确定了我们要获取的信息，我们就可以处理爬虫了。 爬取51job其实非常简单，我们只需要输入指定的关键字，然后点搜索，把返回的网页网址作为起始网址即可。 然后要遵循下面的这个思路： 爬某个页面 把我们所需要的字段信息装载到item里，后面我们要通过pipeline来进行处理 找到下一页的anchor标签的链接地址 如果能找到下一页的话，就递归回到第一步 在解析页面的时候，需要用到scrapy的selector对象的xpath方法。xpath相对于css选择器来说更加复杂一些，但解析效率更高。 1234567891011121314151617181920212223242526272829303132import scrapyfrom jobs.items import JobsItemclass JobspiderSpider(scrapy.Spider): name='jobspider' allowed_domains = ['search.51job.com'] start_urls = [ 'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare='] def parse(self, response): next_page_url = response.xpath('//li[@class="bk"][2]/a/@href') job_list = response.xpath('//*[@id="resultList"]/div[@class="el"]') for each_job in job_list: job_info = JobsItem() job_info['job_title'] = each_job.xpath('.//p[contains(@class,"t1")]/span/a/text()') job_info['company'] = each_job.xpath('.//span[contains(@class,"t2")]/a/text()') job_info['job_href'] = each_job.xpath('.//span[contains(@class,"t2")]/a/@href') job_info['location'] = each_job.xpath('.//span[contains(@class,"t3")]/text()') job_info['salary'] = each_job.xpath('.//span[contains(@class,"t4")]/text()') job_info['post_date'] = each_job.xpath('.//span[contains(@class,"t5")]/text()') # mm-dd for k, v in job_info.items(): if v: job_info[k] = v.extract_first().strip() else: job_info[k] = 'unknown' yield job_info if next_page_url is not None: abs_url = next_page_url.extract_first().strip() yield response.follow(abs_url, callback=self.parse) pipelines.pypipelines用来处理scrapy爬取完页面然后解析出来的item。可以这么理解，pipelines把item处理完之后，会装入到数据库里去。在这里，我们要把数据装入到MYSQL数据库里，并且先不考虑去重等要求。 这需要在MySQL中先建立好DATABASE以及TABLE。 123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-import pymysqlimport datetimeclass JobsPipeline(object): def open_spider(self, spider): self.database = pymysql.connect(host='localhost', port=3306, user='xxxxxx', passwd='xxxxxx', db='job_info', charset='utf8') self.cursor = self.database.cursor() self.table = 'jobs' def process_item(self, item, spider): sql_string = 'INSERT INTO &#123;&#125; (job_title, company, job_href, location, salary, post_date, update_datetime) \ VALUES("&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;",str_to_date("&#123;&#125;","%Y-%m-%d %H:%i:%s"));' \ .format(self.table, item['job_title'], item['company'], item['job_href'], item['location'], item['salary'], item['post_date'], datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')) try: self.cursor.execute(sql_string) self.database.commit() except: print('error') self.database.rollback() return item settings.py完成上面的处理之后，我们还需要做最后一步，修改settings.py，也就是项目设置。 最主要的是两件事情： 设置默认的请求头设置默认的请求头很简单，只需要找到，修改为下面的内容即可。 12345678910DEFAULT_REQUEST_HEADERS = &#123; 'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-US,en;q=0.9', 'Connection':'keep-alive', 'Cookie':'your-cookie-here', 'Host':'search.51job.com', 'Referer':'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,2.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=%27', 'Upgrade-Insecure-Requests':'1', 'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/65.0.3325.181 Chrome/65.0.3325.181 Safari/537.36',&#125; 那上面这些内容是从哪来的呢？当然是从Chrome浏览器的Request Headers的信息拿出来的。 开启pipelines这个也很简单，把下面这句取消注释即可。 123ITEM_PIPELINES = &#123; 'jobs.pipelines.JobsPipeline': 300,&#125; 最终运行cd到项目的根目录下，注意一定要是根目录，然后执行： 1scrapy crawl jobspider 至此，一个最简单的Scrapy爬虫就写好了。后期还有很多细节要处理，比如数据去重，异常处理，数据分析等等，不过这些我们可以在后面优化，后面我们也会陆续讲解。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
</search>
