<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[用Excel计算还款之理论基础]]></title>
    <url>%2F2018%2F06%2F23%2Fexcel-loan-repayment-basic%2F</url>
    <content type="text"><![CDATA[​ 新开一部分专题，来讲讲Excel在金融建模中的应用。 计划在这个专题里，跟随经典的教材，讲一些日常生活中我们常见的可以和财务以及Excel结合的例子。一方面，可以提高自己的金融知识水平，另一方面，也可以顺手解决一些实际需求。 今天先从还款，也就是loan repayment讲起。 话说小明毕业后来到魔都，经过5年奋斗，终于省吃俭用的攒下了10万块钱。加上家里父母支持的90万，凑够了100万首付。他看中的房子标价250万，他还需要贷款150万。 问题来了，如果小明商业贷款的利率是每年4.9%，计划30年还清，那他每个月需要还多少月供？ 在开始计算之前，我们需要弄清楚偿还贷款的两种方式：等额本金和等额本息法。 等额本金：Even Principal Payments等额本息： Even Total Payments 等额本金等额本金的概念是：每个月的还款包括两部分：本金部分和利息部分。每次还款中的本金部分的数额都是相同的，而利息部分则等于剩余未偿还的本金所产生的利息。以上面的例子为例： 贷款总额150万，360个月偿还，每个月需要还1500000/360=4167元本金 第一个月本金基数为150万，所产生的利息为：1500000*0.049/12=6125 所以第一个月的偿还总额为：4167+6125=10292 到了第2个月 需要偿还的本金依旧是4167元 因为上一个月已经还了4167元本金，因此剩余的本金产生的利息为：(1500000-4167)*0.049/12=6108 所以第二个月的还款总额是：4167+6108=10275 以此类推，一直到最后一个月。 算一算我们就看出来了，等额本金法每个月的还款额是逐渐减少的。每个月减少的数额实际上就是上个月偿还的本金额一个月所产生的利息。用上面的例子来计算，就是4167*0.049/12=17。 用Excel模拟计算出来的结果如下： 等额本息等额本息的概念是：每期还款的数额是不变的。每期还款后，剩下的本金加上当期产生的利息作为一个整体继续计息。 这种方式就是利滚利 假设每期的利率为r贷款金额为p 第一期还款x元，剩余欠款为p(1+r)-x 第二期还款x元，剩余欠款为(p(1+r)-x)*(1+r)-x = p(1+r)^2 -x(1+r) -x 第三期还款x元，剩余欠款为p(1+r)^3- x(1+r)^2- x(1+r) -x 第n期还款x元，剩余欠款为 p(1+r)^n - x(1+r)^(n-1) -x(1+r)^(n-2) - ... - x(1+r) -x 到了第n期，剩余欠款就为0了，因此 p(1+r)^n - x(1+r)^(n-1) -x(1+r)^(n-2) - … - x(1+r) -x = 0利用上面这个公式，以及等比公式求和公式，我们可以推导出： 回到我们题目中的例子，如果采用等额本息的方法，每个月的还款额为：1500000*0.049/12*(1+0.049/12)^360/((1+0.049/12)^360-1)=7691 用Excel模拟出来的结果如下： 两种方法的比较 从实际还款总额来看的话，等额本金法比等额本息法要少。我们上面这个例子，等额本金最终所有还款额为260万，而等额本息的还款总额为286万 从每期还款数目来看的话，等额本金法最初的单期还款数额比等额本息要高很多，初期的资金压力会比较大，但其还款金额会随着时间递减 上面公式看起来都蛮复杂的，那么怎么样用Excel来计算呢？这个我们在下一节继续研究。]]></content>
      <categories>
        <category>金融</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同样的Access，不一样的数据类型]]></title>
    <url>%2F2018%2F06%2F21%2FAccess-version-data-type%2F</url>
    <content type="text"><![CDATA[​ 最近算是和ACCESS彻底杠上了。 其实我一般是不太喜欢用ACCESS的，因为自己不是专门做数据库的，用ACCESS总有一种杀鸡用牛刀的感觉。 不过最近做一个项目，指定要用ACCESS。于是在自己的私人电脑上做了一个小的demo文件。然而，这个文件拷贝到公司电脑上，打开时却报错了： 看这个报错，貌似是因为公司电脑的ACCESS版本不够高？查看了一下公司电脑的版本，已经是2016版了。难道还有比2016版还要高的版本？ 还是老套路，到MSDN上面搜一个报错关键词，查到这个页面，里面提到： This issue occurs if you try to open a database that contains a table that uses the BigInt data type. BigInt support was added to Access 2016. BigInt appears within the table’s data type as Large Number. 也就是说，ACCESS2016版里新增了一个数据类型叫BigInt。如果用以前的ACCESS版本来打开含有BigInt类型的文件，就会报错。 那么问题来了，我的公司电脑已经是2016版的，里面有没有Large Number这个数据类型呢？看了一下，居然没有！为什么呢？ 还有一点：MSDN里说，如果要打开的表格里使用了BigInt这个类型才会报错，可是我的原始文件里也没有用到这个数据类型，为什么也会报错呢？ 这两个问题始终没有得到答案。最后的处理方式是：把私人电脑里的ACCESS数据导出为Excel文件，然后导入到公司电脑上。这也算是一种曲线救国的方式了。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Access</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用ADO导入CSV后部分数据变成NULL，怎么破？]]></title>
    <url>%2F2018%2F06%2F20%2FADO-Text-Driver-Data-Type%2F</url>
    <content type="text"><![CDATA[​ 最近要处理一些数据量比较大的CSV文件，本着能不用ACCESS就不用ACCESS的原则，使用了VBA+ADO联用的方法。 CSV，Comma Separated Value，是一种用逗号分隔不同的值，来保存数据的文本文件 ADO, Active Data Object，可以简单的看作是操作数据库的接口 出现了什么问题？首先要做的一步是把CSV中的数据读取到工作表里。鉴于这不是一篇入门的文章，我们就不在这里介绍基础内容了。 现有的p.csv文件是这个样子： 用下面的这段程序，可以直接把CSV中的数据读入到工作表里。 12345678910111213141516171819Option ExplicitSub main() LoadData "p.csv"End SubPrivate Sub LoadData(strCSVPath As String) Dim strSQL As String, strConnection As String Dim oRst As ADODB.Recordset strConnection = "Provider=Microsoft.ace.oledb.12.0;" &amp; _ "Data Source=" &amp; ThisWorkbook.Path &amp; ";" &amp; _ "Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';" Set oRst = New ADODB.Recordset strSQL = "SELECT * FROM " &amp; strCSVPath oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly Sheet1.Range("a1").CopyFromRecordset oRst oRst.Close Set oRst = NothingEnd Sub 上面这段代码，可以把当前工作簿目录下的p.csv中的数据内容导入到sheet1工作表里。查看sheet1工作表，发现一个问题：有些单元格是空的，比如第189，195,196行。 再返回到源文件查看对应的行，第一眼看上去是有数据的啊，见下图： 又重复运行了几次脚本，复制出来的数据始终有空行，百思不得其解。 原因在哪里？又盯着看了一会，似乎看出了点端倪。不能被显示出来的数据貌似都很大，都是E9数量级。但看第188行，1.22E9是可以正常显示的。 感觉显示为空单元的数字都太大了，所以没有显示出来。经过LEAF大神的提醒，可以使用Fields.Type来查看数据类型。上面的代码修改为： 1234567891011121314151617181920212223242526Option ExplicitSub main() LoadData "p.csv"End SubPrivate Sub LoadData(strCSVPath As String) Dim strSQL As String, strConnection As String Dim oRst As ADODB.Recordset Dim i As Integer strConnection = "Provider=Microsoft.ace.oledb.12.0;" &amp; _ "Data Source=" &amp; ThisWorkbook.Path &amp; ";" &amp; _ "Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';" Set oRst = New ADODB.Recordset strSQL = "SELECT * FROM " &amp; strCSVPath oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly '以下为新增代码 '======== For i = 0 To oRst.Fields.Count - 1 Debug.Print oRst.Fields(i).Type, oRst.Fields(i).Name Next i '======== '以上为新增代码 Sheet1.Range("a1").CopyFromRecordset oRst oRst.Close Set oRst = NothingEnd Sub 到立即窗口查看，打印出了下面的内容：123202 账户简称3 金额（元）7 结算日期 也就是说，金额字段对应的数据类型是3。数据类型3又是什么呢？直接在msdn上搜索一下，发现这个页面里说的很清楚： 3对应的是4个字节的正负整数。4个字节大约可以表示从-2^31到2^31之间，大约是-2.15E9到2.15E9。难怪4.7E9显示不出来，因为变量长度溢出了啊。 为什么会出现这个问题？在用ADO读取文本文件的时候，因为文本文件中的数据格式没有被显式指定，所以ADO的文本引擎会先扫描一定的行数，然后给每列推测一个数据类型。 很显然，文中出现的问题就是因为ADO扫描完之后，指定了一个adInteger类型，但后面出现的大的数字已经超出了范围。 那么，ADO会扫描多少行呢？在网上搜了一下，在这个注册表下面有ADO文本引擎的默认设置：[Computer\HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\Jet\4.0\Engines\Text] 很明显，MaxScanRows的值是25，也就是说Text引擎会扫描25行，然后根据扫描结果指定一个数据类型。 怎么解决？有两种方法。 第一种方法不太推荐，就是直接修改注册表，把MaxScanRows的值修改为足够大。如果想扫描全部内容，就把MaxScanRows的值改为0。 第二种方法更加安全一些，也就是用一个文件Schema.ini来存储文本文件的信息。 Schema.ini一定要与CSV文件在同一个目录下，他包含了下面这些信息： 文件名 文件格式 字段名 字符集 等等 详细的格式设置可以查看这个链接 在这个例子里，我们就在p.csv所在的文件夹创建Schema.ini，然后简单的指定下面的内容即可。 1234[p.csv]Format = CSVDelimitedColNameHeader = TrueMaxScanRows = 0 当然，我们也可以把创建文件的内容加入到ADO脚本里，最后完整的脚本内容就会变成： 12345678910111213141516171819202122232425262728293031323334353637Option ExplicitSub main() CreateIniFile "p.csv" LoadData "p.csv"End SubPrivate Sub LoadData(strCSVPath As String) Dim strSQL As String, strConnection As String Dim oRst As ADODB.Recordset Dim i As Integer strConnection = "Provider=Microsoft.ace.oledb.12.0;" &amp; _ "Data Source=" &amp; ThisWorkbook.Path &amp; ";" &amp; _ "Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';" Set oRst = New ADODB.Recordset strSQL = "SELECT * FROM " &amp; strCSVPath oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly Sheet1.Range("a1").CopyFromRecordset oRst oRst.Close Set oRst = NothingEnd SubPrivate Sub CreateIniFile(strCSVPath As String) Dim iFreeFile As Integer Dim StrIni As String StrIni = "[" &amp; strCSVPath &amp; "]" &amp; vbCrLf &amp; _ "Format = CSVDelimited" &amp; vbCrLf &amp; _ "ColNameHeader = True" &amp; vbCrLf &amp; _ "MaxScanRows = 0" iFreeFile = FreeFile Open ThisWorkbook.Path &amp; "\Schema.ini" For Output As #iFreeFile Print #iFreeFile, StrIni Close #iFreeFileEnd Sub 再次运行程序，发现之前的空白单元格已经有数字进来了： 总结 VBA还是要多查MSDN ADO里的Field的基本属性要熟练]]></content>
      <categories>
        <category>VBA</category>
      </categories>
      <tags>
        <tag>ADO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用scrapy爬取职位信息]]></title>
    <url>%2F2018%2F06%2F15%2F%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[​ 新开一篇，来讲解一个51JOB工作职位的爬取。目标：爬取51JOB上所有与化工相关的职位信息。 工具：scrapy框架+Python3 第一步：安装环境使用： 1virtualenv -p /usr/bin/python3 scrapy_env 来创建一个新的虚拟环境来运行scrapy cd到scrapy_env文件夹，然后执行下面语句来开启虚拟空间： 1~/scrapy_env$ source bin/activate 进入到scrapy_env的虚拟空间后，用pip3 install scrapy来安装scrapy所需要的依赖 创建Scrapy项目使用scrapy startproject来创建项目。 12345678910$ scrapy startproject 51jobError: Project names must begin with a letter and contain onlyletters, numbers and underscores~$ scrapy startproject jobsNew Scrapy project 'jobs', using template directory '/home/ubuntu/scrapy_env/lib/python3.5/site-packages/scrapy/templates/project', created in: /home/ubuntu/jobsYou can start your first spider with: cd jobs scrapy genspider example example.com 然后使用scrapy genspider来创建一个爬虫 123~/jobs$ scrapy genspider JobSpider search.51job.comCreated spider 'JobSpider' using template 'basic' in module: jobs.spiders.JobSpider 这样，项目的框架就搭好了。现在整个项目的目录文件应该是这样子的： 123456789101112131415.├── jobs│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── __pycache__│ │ ├── __init__.cpython-35.pyc│ │ └── settings.cpython-35.pyc│ ├── settings.py│ └── spiders│ ├── __init__.py│ ├── JobSpider.py│ └── __pycache__└── scrapy.cfg 编辑Scrapy文件Scrapy框架极大程度的减少了我们爬虫额代码编写量。对于一个简单的爬虫，我们只需要修改下面几个内容。 首次得打开网页链接：1https://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare= items.py我们需要获取的每一条招聘内容包括了下面这些信息： 职位名 公司名 工作地点 薪资 发布时间因此，我们需要根据这些来定义一个Item，也就是一条爬取下来的信息。 1234567891011import scrapyclass JobsItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() job_title = scrapy.Field() company = scrapy.Field() job_href = scrapy.Field() location = scrapy.Field() salary = scrapy.Field() post_date = scrapy.Field() 这就好了，讲白了，我们是定义了一个名字叫JobItem的字典，然后给这个字典定义了5个键。 spiders/JobSpider.py确定了我们要获取的信息，我们就可以处理爬虫了。 爬取51job其实非常简单，我们只需要输入指定的关键字，然后点搜索，把返回的网页网址作为起始网址即可。 然后要遵循下面的这个思路： 爬某个页面 把我们所需要的字段信息装载到item里，后面我们要通过pipeline来进行处理 找到下一页的anchor标签的链接地址 如果能找到下一页的话，就递归回到第一步 在解析页面的时候，需要用到scrapy的selector对象的xpath方法。xpath相对于css选择器来说更加复杂一些，但解析效率更高。 1234567891011121314151617181920212223242526272829303132import scrapyfrom jobs.items import JobsItemclass JobspiderSpider(scrapy.Spider): name='jobspider' allowed_domains = ['search.51job.com'] start_urls = [ 'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare='] def parse(self, response): next_page_url = response.xpath('//li[@class="bk"][2]/a/@href') job_list = response.xpath('//*[@id="resultList"]/div[@class="el"]') for each_job in job_list: job_info = JobsItem() job_info['job_title'] = each_job.xpath('.//p[contains(@class,"t1")]/span/a/text()') job_info['company'] = each_job.xpath('.//span[contains(@class,"t2")]/a/text()') job_info['job_href'] = each_job.xpath('.//span[contains(@class,"t2")]/a/@href') job_info['location'] = each_job.xpath('.//span[contains(@class,"t3")]/text()') job_info['salary'] = each_job.xpath('.//span[contains(@class,"t4")]/text()') job_info['post_date'] = each_job.xpath('.//span[contains(@class,"t5")]/text()') # mm-dd for k, v in job_info.items(): if v: job_info[k] = v.extract_first().strip() else: job_info[k] = 'unknown' yield job_info if next_page_url is not None: abs_url = next_page_url.extract_first().strip() yield response.follow(abs_url, callback=self.parse) pipelines.pypipelines用来处理scrapy爬取完页面然后解析出来的item。可以这么理解，pipelines把item处理完之后，会装入到数据库里去。在这里，我们要把数据装入到MYSQL数据库里，并且先不考虑去重等要求。 这需要在MySQL中先建立好DATABASE以及TABLE。 123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-import pymysqlimport datetimeclass JobsPipeline(object): def open_spider(self, spider): self.database = pymysql.connect(host='localhost', port=3306, user='xxxxxx', passwd='xxxxxx', db='job_info', charset='utf8') self.cursor = self.database.cursor() self.table = 'jobs' def process_item(self, item, spider): sql_string = 'INSERT INTO &#123;&#125; (job_title, company, job_href, location, salary, post_date, update_datetime) \ VALUES("&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;",str_to_date("&#123;&#125;","%Y-%m-%d %H:%i:%s"));' \ .format(self.table, item['job_title'], item['company'], item['job_href'], item['location'], item['salary'], item['post_date'], datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')) try: self.cursor.execute(sql_string) self.database.commit() except: print('error') self.database.rollback() return item settings.py完成上面的处理之后，我们还需要做最后一步，修改settings.py，也就是项目设置。 最主要的是两件事情： 设置默认的请求头设置默认的请求头很简单，只需要找到，修改为下面的内容即可。 12345678910DEFAULT_REQUEST_HEADERS = &#123; 'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-US,en;q=0.9', 'Connection':'keep-alive', 'Cookie':'your-cookie-here', 'Host':'search.51job.com', 'Referer':'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,2.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=%27', 'Upgrade-Insecure-Requests':'1', 'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/65.0.3325.181 Chrome/65.0.3325.181 Safari/537.36',&#125; 那上面这些内容是从哪来的呢？当然是从Chrome浏览器的Request Headers的信息拿出来的。 开启pipelines这个也很简单，把下面这句取消注释即可。 123ITEM_PIPELINES = &#123; 'jobs.pipelines.JobsPipeline': 300,&#125; 最终运行cd到项目的根目录下，注意一定要是根目录，然后执行： 1scrapy crawl jobspider 至此，一个最简单的Scrapy爬虫就写好了。后期还有很多细节要处理，比如数据去重，异常处理，数据分析等等，不过这些我们可以在后面优化，后面我们也会陆续讲解。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
</search>
