<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>用Excel计算还款之理论基础</title>
      <link href="/2018/06/23/excel-loan-repayment-basic/"/>
      <url>/2018/06/23/excel-loan-repayment-basic/</url>
      <content type="html"><![CDATA[<p>​    </p><a id="more"></a><p>新开一部分专题，来讲讲Excel在金融建模中的应用。  </p><p>计划在这个专题里，跟随经典的教材，讲一些日常生活中我们常见的可以和财务以及Excel结合的例子。一方面，可以提高自己的金融知识水平，另一方面，也可以顺手解决一些实际需求。  </p><p>今天先从还款，也就是<code>loan repayment</code>讲起。  </p><p>话说小明毕业后来到魔都，经过5年奋斗，终于省吃俭用的攒下了10万块钱。加上家里父母支持的90万，凑够了100万首付。他看中的房子标价250万，他还需要贷款150万。  </p><p>问题来了，如果小明商业贷款的利率是每年4.9%，计划30年还清，那他每个月需要还多少月供？  </p><p>在开始计算之前，我们需要弄清楚偿还贷款的两种方式：等额本金和等额本息法。  </p><blockquote><p>等额本金：Even Principal Payments<br>等额本息： Even Total Payments  </p></blockquote><h2 id="等额本金"><a href="#等额本金" class="headerlink" title="等额本金"></a>等额本金</h2><p>等额本金的概念是：每个月的还款包括两部分：本金部分和利息部分。<br>每次还款中的本金部分的数额都是相同的，而利息部分则等于<br>剩余未偿还的本金所产生的利息。<br>以上面的例子为例：  </p><ul><li>贷款总额150万，360个月偿还，每个月需要还<code>1500000/360=4167</code>元本金</li><li>第一个月本金基数为150万，所产生的利息为：<code>1500000*0.049/12=6125</code></li><li>所以第一个月的偿还总额为：<code>4167+6125=10292</code>  </li></ul><p>到了第2个月</p><ul><li>需要偿还的本金依旧是<code>4167</code>元</li><li>因为上一个月已经还了<code>4167</code>元本金，因此剩余的本金产生的利息为：<code>(1500000-4167)*0.049/12=6108</code></li><li>所以第二个月的还款总额是：<code>4167+6108=10275</code>  </li></ul><p>以此类推，一直到最后一个月。  </p><p>算一算我们就看出来了，等额本金法每个月的还款额是逐渐减少的。<br>每个月减少的数额实际上就是上个月偿还的本金额一个月所产生的利息。用上面的例子来计算，就是<code>4167*0.049/12=17</code>。  </p><p>用Excel模拟计算出来的结果如下：<br><img src="http://pah1qyen2.bkt.clouddn.com/blog/180623/dkK69Ajbj0.png?imageslim" alt="mark"></p><h2 id="等额本息"><a href="#等额本息" class="headerlink" title="等额本息"></a>等额本息</h2><p>等额本息的概念是：每期还款的数额是不变的。每期还款后，剩下的本金加上当期产生的利息作为一个整体继续计息。  </p><blockquote><p>这种方式就是利滚利  </p></blockquote><p>假设每期的利率为r<br>贷款金额为p</p><ul><li>第一期还款x元，剩余欠款为<code>p(1+r)-x</code></li><li>第二期还款x元，剩余欠款为<code>(p(1+r)-x)*(1+r)-x = p(1+r)^2 -x(1+r) -x</code></li><li>第三期还款x元，剩余欠款为<code>p(1+r)^3- x(1+r)^2- x(1+r) -x</code></li><li>第n期还款x元，剩余欠款为 <code>p(1+r)^n - x(1+r)^(n-1) -x(1+r)^(n-2) - ... - x(1+r) -x</code></li></ul><p>到了第n期，剩余欠款就为0了，因此 p(1+r)^n - x(1+r)^(n-1) -x(1+r)^(n-2) - … - x(1+r) -x = 0<br>利用上面这个公式，以及等比公式求和公式，我们可以推导出：  </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180623/EgJ0K132CG.png?imageslim" alt="mark">  </p><p>回到我们题目中的例子，如果采用等额本息的方法，每个月的还款额为：<code>1500000*0.049/12*(1+0.049/12)^360/((1+0.049/12)^360-1)=7691</code>    </p><p>用Excel模拟出来的结果如下：   </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180623/FKf16HDEg4.png?imageslim" alt="mark"></p><h2 id="两种方法的比较"><a href="#两种方法的比较" class="headerlink" title="两种方法的比较"></a>两种方法的比较</h2><ul><li>从实际还款总额来看的话，等额本金法比等额本息法要少。我们上面这个例子，等额本金最终所有还款额为260万，而等额本息的还款总额为286万  </li><li>从每期还款数目来看的话，等额本金法最初的单期还款数额比等额本息要高很多，初期的资金压力会比较大，但其还款金额会随着时间递减</li></ul><p>上面公式看起来都蛮复杂的，那么怎么样用Excel来计算呢？这个我们在下一节继续研究。  </p>]]></content>
      
      <categories>
          
          <category> 金融 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Excel </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>同样的Access，不一样的数据类型</title>
      <link href="/2018/06/21/Access-version-data-type/"/>
      <url>/2018/06/21/Access-version-data-type/</url>
      <content type="html"><![CDATA[<p>​    </p><a id="more"></a><p>最近算是和<code>ACCESS</code>彻底杠上了。  </p><p>其实我一般是不太喜欢用<code>ACCESS</code>的，因为自己不是专门做数据库的，用<code>ACCESS</code>总有一种杀鸡用牛刀的感觉。  </p><p>不过最近做一个项目，指定要用<code>ACCESS</code>。于是在自己的私人电脑上做了一个小的demo文件。<br>然而，这个文件拷贝到公司电脑上，打开时却报错了：  </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180621/FJc7mJGKb0.png?imageslim" alt="mark"></p><p>看这个报错，貌似是因为公司电脑的<code>ACCESS</code>版本不够高？查看了一下公司电脑的版本，已经是2016版了。难道还有比2016版还要高的版本？ </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180621/jhfjdHjCCk.png?imageslim" alt="mark">  </p><p>还是老套路，到<code>MSDN</code>上面搜一个报错关键词，查到<a href="https://support.microsoft.com/en-gb/help/3208802/database-you-are-trying-to-open-requires-a-newer-version-of-microsoft" target="_blank" rel="noopener">这个页面</a>，里面提到：  </p><blockquote><p>This issue occurs if you try to open a database that contains a table that uses the BigInt data type. BigInt support was added to Access 2016. BigInt appears within the table’s data type as Large Number.</p></blockquote><p>也就是说，<code>ACCESS</code>2016版里新增了一个数据类型叫<code>BigInt</code>。如果用以前的<code>ACCESS</code>版本来打开含有<code>BigInt</code>类型的文件，就会报错。   </p><p>那么问题来了，我的公司电脑已经是2016版的，里面有没有<code>Large Number</code>这个数据类型呢？看了一下，居然没有！为什么呢？</p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180621/3hAfah8gC6.png?imageslim" alt="mark">  </p><p>还有一点：<code>MSDN</code>里说，如果要打开的表格里使用了<code>BigInt</code>这个类型才会报错，可是我的原始文件里也没有用到这个数据类型，为什么也会报错呢？  </p><p>这两个问题始终没有得到答案。最后的处理方式是：把私人电脑里的<code>ACCESS</code>数据导出为Excel文件，然后导入到公司电脑上。这也算是一种曲线救国的方式了。  </p>]]></content>
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Access </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用ADO导入CSV后部分数据变成NULL，怎么破？</title>
      <link href="/2018/06/20/ADO-Text-Driver-Data-Type/"/>
      <url>/2018/06/20/ADO-Text-Driver-Data-Type/</url>
      <content type="html"><![CDATA[<p>​    </p><a id="more"></a><p>最近要处理一些数据量比较大的CSV文件，本着能不用<code>ACCESS</code>就不用<code>ACCESS</code>的原则，使用了<code>VBA</code>+<code>ADO</code>联用的方法。  </p><blockquote><p>CSV，Comma Separated Value，是一种用逗号分隔不同的值，来保存数据的文本文件  </p></blockquote><blockquote><p>ADO, Active Data Object，可以简单的看作是操作数据库的接口 </p></blockquote><h2 id="出现了什么问题？"><a href="#出现了什么问题？" class="headerlink" title="出现了什么问题？"></a>出现了什么问题？</h2><p>首先要做的一步是把<code>CSV</code>中的数据读取到工作表里。鉴于这不是一篇入门的文章，我们就不在这里介绍基础内容了。  </p><p>现有的<code>p.csv</code>文件是这个样子：    </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180620/FJJ1bKaeH4.png?imageslim" alt="mark"></p><p>用下面的这段程序，可以直接把<code>CSV</code>中的数据读入到工作表里。  </p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Option</span> <span class="keyword">Explicit</span></span><br><span class="line"><span class="keyword">Sub</span> main()</span><br><span class="line">    LoadData <span class="string">"p.csv"</span></span><br><span class="line"><span class="keyword">End</span> <span class="keyword">Sub</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">Private</span> <span class="keyword">Sub</span> LoadData(strCSVPath <span class="keyword">As</span> <span class="built_in">String</span>)</span><br><span class="line">    <span class="keyword">Dim</span> strSQL <span class="keyword">As</span> <span class="built_in">String</span>, strConnection <span class="keyword">As</span> <span class="built_in">String</span></span><br><span class="line">    <span class="keyword">Dim</span> oRst <span class="keyword">As</span> ADODB.Recordset</span><br><span class="line">    </span><br><span class="line">    strConnection = <span class="string">"Provider=Microsoft.ace.oledb.12.0;"</span> &amp; _</span><br><span class="line">                    <span class="string">"Data Source="</span> &amp; ThisWorkbook.Path &amp; <span class="string">";"</span> &amp; _</span><br><span class="line">                    <span class="string">"Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';"</span></span><br><span class="line">    <span class="keyword">Set</span> oRst = <span class="keyword">New</span> ADODB.Recordset</span><br><span class="line">    strSQL = <span class="string">"SELECT * FROM "</span> &amp; strCSVPath</span><br><span class="line">    oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly</span><br><span class="line">    Sheet1.Range(<span class="string">"a1"</span>).CopyFromRecordset oRst</span><br><span class="line">    oRst.Close</span><br><span class="line">    <span class="keyword">Set</span> oRst = <span class="literal">Nothing</span></span><br><span class="line"><span class="keyword">End</span> <span class="keyword">Sub</span></span><br></pre></td></tr></table></figure><p>上面这段代码，可以把当前工作簿目录下的<code>p.csv</code>中的数据内容导入到sheet1工作表里。<br>查看sheet1工作表，发现一个问题：有些单元格是空的，比如第189，195,196行。  </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180620/cfh4lme6fK.png?imageslim" alt="mark"></p><p>再返回到源文件查看对应的行，第一眼看上去是有数据的啊，见下图：</p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180620/9cklHmjD3a.png?imageslim" alt="mark">  </p><p>又重复运行了几次脚本，复制出来的数据始终有空行，百思不得其解。  </p><h2 id="原因在哪里？"><a href="#原因在哪里？" class="headerlink" title="原因在哪里？"></a>原因在哪里？</h2><p>又盯着看了一会，似乎看出了点端倪。不能被显示出来的数据貌似都很大，都是<code>E9</code>数量级。但看第188行，<code>1.22E9</code>是可以正常显示的。  </p><p>感觉显示为空单元的数字都太大了，所以没有显示出来。经过LEAF大神的提醒，可以使用<code>Fields.Type</code>来查看数据类型。上面的代码修改为：</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Option</span> <span class="keyword">Explicit</span></span><br><span class="line"><span class="keyword">Sub</span> main()</span><br><span class="line">    LoadData <span class="string">"p.csv"</span></span><br><span class="line"><span class="keyword">End</span> <span class="keyword">Sub</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">Private</span> <span class="keyword">Sub</span> LoadData(strCSVPath <span class="keyword">As</span> <span class="built_in">String</span>)</span><br><span class="line">    <span class="keyword">Dim</span> strSQL <span class="keyword">As</span> <span class="built_in">String</span>, strConnection <span class="keyword">As</span> <span class="built_in">String</span></span><br><span class="line">    <span class="keyword">Dim</span> oRst <span class="keyword">As</span> ADODB.Recordset</span><br><span class="line">    <span class="keyword">Dim</span> i <span class="keyword">As</span> <span class="built_in">Integer</span></span><br><span class="line">    strConnection = <span class="string">"Provider=Microsoft.ace.oledb.12.0;"</span> &amp; _</span><br><span class="line">                    <span class="string">"Data Source="</span> &amp; ThisWorkbook.Path &amp; <span class="string">";"</span> &amp; _</span><br><span class="line">                    <span class="string">"Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';"</span></span><br><span class="line">    <span class="keyword">Set</span> oRst = <span class="keyword">New</span> ADODB.Recordset</span><br><span class="line">    strSQL = <span class="string">"SELECT * FROM "</span> &amp; strCSVPath</span><br><span class="line">    oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly</span><br><span class="line">    <span class="comment">'以下为新增代码</span></span><br><span class="line">    <span class="comment">'========</span></span><br><span class="line">    <span class="keyword">For</span> i = <span class="number">0</span> <span class="keyword">To</span> oRst.Fields.Count - <span class="number">1</span></span><br><span class="line">        Debug.Print oRst.Fields(i).Type, oRst.Fields(i).Name</span><br><span class="line">    <span class="keyword">Next</span> i</span><br><span class="line">    <span class="comment">'========</span></span><br><span class="line">    <span class="comment">'以上为新增代码</span></span><br><span class="line">    Sheet1.Range(<span class="string">"a1"</span>).CopyFromRecordset oRst</span><br><span class="line">    oRst.Close</span><br><span class="line">    <span class="keyword">Set</span> oRst = <span class="literal">Nothing</span></span><br><span class="line"><span class="keyword">End</span> <span class="keyword">Sub</span></span><br></pre></td></tr></table></figure><p>到立即窗口查看，打印出了下面的内容：<br><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">202 </span>      账户简称</span><br><span class="line"><span class="symbol">3 </span>        金额（元）</span><br><span class="line"><span class="symbol">7 </span>        结算日期</span><br></pre></td></tr></table></figure></p><p>也就是说，金额字段对应的数据类型是3。数据类型3又是什么呢？直接在msdn上搜索一下，发现<a href="https://docs.microsoft.com/en-us/sql/ado/reference/ado-api/datatypeenum?view=sql-server-2017" target="_blank" rel="noopener">这个页面</a>里说的很清楚：  </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180620/i22f12jchA.png?imageslim" alt="mark">  </p><p>3对应的是4个字节的正负整数。4个字节大约可以表示从<code>-2^31</code>到<code>2^31</code>之间，大约是<code>-2.15E9</code>到<code>2.15E9</code>。难怪<code>4.7E9</code>显示不出来，因为变量长度溢出了啊。  </p><h2 id="为什么会出现这个问题？"><a href="#为什么会出现这个问题？" class="headerlink" title="为什么会出现这个问题？"></a>为什么会出现这个问题？</h2><p>在用ADO读取文本文件的时候，因为文本文件中的数据格式没有被显式指定，所以ADO的文本引擎会先扫描一定的行数，然后给每列推测一个数据类型。  </p><p>很显然，文中出现的问题就是因为ADO扫描完之后，指定了一个<code>adInteger</code>类型，但后面出现的大的数字已经超出了范围。  </p><p>那么，ADO会扫描多少行呢？<br>在网上搜了一下，在这个注册表下面有ADO文本引擎的默认设置：<br><code>[Computer\HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\Jet\4.0\Engines\Text]</code></p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180620/Db41eFFBbK.png?imageslim" alt="mark">  </p><p>很明显，MaxScanRows的值是25，也就是说Text引擎会扫描25行，然后根据扫描结果指定一个数据类型。  </p><h2 id="怎么解决？"><a href="#怎么解决？" class="headerlink" title="怎么解决？"></a>怎么解决？</h2><p>有两种方法。  </p><p>第一种方法不太推荐，就是直接修改注册表，把<code>MaxScanRows</code>的值修改为足够大。如果想扫描全部内容，就把<code>MaxScanRows</code>的值改为0。  </p><p>第二种方法更加安全一些，也就是用一个文件<code>Schema.ini</code>来存储文本文件的信息。  </p><p><code>Schema.ini</code>一定要与<code>CSV</code>文件在同一个目录下，他包含了下面这些信息：  </p><ul><li>文件名  </li><li>文件格式  </li><li>字段名</li><li>字符集</li><li>等等  </li></ul><p>详细的格式设置可以查看<a href="https://docs.microsoft.com/en-us/sql/odbc/microsoft/schema-ini-file-text-file-driver?view=sql-server-2017" target="_blank" rel="noopener">这个链接</a>  </p><p>在这个例子里，我们就在<code>p.csv</code>所在的文件夹创建<code>Schema.ini</code>，然后简单的指定下面的内容即可。  </p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[p.csv]</span></span><br><span class="line"><span class="attr">Format</span> = CSVDelimited</span><br><span class="line"><span class="attr">ColNameHeader</span> = <span class="literal">True</span></span><br><span class="line"><span class="attr">MaxScanRows</span> = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>当然，我们也可以把创建文件的内容加入到<code>ADO</code>脚本里，最后完整的脚本内容就会变成：  </p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Option</span> <span class="keyword">Explicit</span></span><br><span class="line"><span class="keyword">Sub</span> main()</span><br><span class="line">    CreateIniFile <span class="string">"p.csv"</span></span><br><span class="line">    LoadData <span class="string">"p.csv"</span></span><br><span class="line"><span class="keyword">End</span> <span class="keyword">Sub</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">Private</span> <span class="keyword">Sub</span> LoadData(strCSVPath <span class="keyword">As</span> <span class="built_in">String</span>)</span><br><span class="line">    <span class="keyword">Dim</span> strSQL <span class="keyword">As</span> <span class="built_in">String</span>, strConnection <span class="keyword">As</span> <span class="built_in">String</span></span><br><span class="line">    <span class="keyword">Dim</span> oRst <span class="keyword">As</span> ADODB.Recordset</span><br><span class="line">    <span class="keyword">Dim</span> i <span class="keyword">As</span> <span class="built_in">Integer</span></span><br><span class="line">    strConnection = <span class="string">"Provider=Microsoft.ace.oledb.12.0;"</span> &amp; _</span><br><span class="line">                    <span class="string">"Data Source="</span> &amp; ThisWorkbook.Path &amp; <span class="string">";"</span> &amp; _</span><br><span class="line">                    <span class="string">"Extended Properties= 'text;HDR=Yes;IMEX=1;FMT=Delimited(,)';"</span></span><br><span class="line">    <span class="keyword">Set</span> oRst = <span class="keyword">New</span> ADODB.Recordset</span><br><span class="line">    strSQL = <span class="string">"SELECT * FROM "</span> &amp; strCSVPath</span><br><span class="line">    oRst.Open strSQL, strConnection, adOpenStatic, adLockReadOnly</span><br><span class="line">    Sheet1.Range(<span class="string">"a1"</span>).CopyFromRecordset oRst</span><br><span class="line">    oRst.Close</span><br><span class="line">    <span class="keyword">Set</span> oRst = <span class="literal">Nothing</span></span><br><span class="line"><span class="keyword">End</span> <span class="keyword">Sub</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">Private</span> <span class="keyword">Sub</span> CreateIniFile(strCSVPath <span class="keyword">As</span> <span class="built_in">String</span>)</span><br><span class="line">    <span class="keyword">Dim</span> iFreeFile <span class="keyword">As</span> <span class="built_in">Integer</span></span><br><span class="line">    <span class="keyword">Dim</span> StrIni <span class="keyword">As</span> <span class="built_in">String</span></span><br><span class="line">    </span><br><span class="line">    StrIni = <span class="string">"["</span> &amp; strCSVPath &amp; <span class="string">"]"</span> &amp; vbCrLf &amp; _</span><br><span class="line">                    <span class="string">"Format = CSVDelimited"</span> &amp; vbCrLf &amp; _</span><br><span class="line">                    <span class="string">"ColNameHeader = True"</span> &amp; vbCrLf &amp; _</span><br><span class="line">                    <span class="string">"MaxScanRows = 0"</span></span><br><span class="line">    </span><br><span class="line">    iFreeFile = FreeFile</span><br><span class="line">    </span><br><span class="line">    Open ThisWorkbook.Path &amp; <span class="string">"\Schema.ini"</span> <span class="keyword">For</span> Output <span class="keyword">As</span> <span class="meta">#iFreeFile</span></span><br><span class="line">    Print <span class="meta">#iFreeFile, StrIni</span></span><br><span class="line">    Close <span class="meta">#iFreeFile</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">End</span> <span class="keyword">Sub</span></span><br></pre></td></tr></table></figure><p>再次运行程序，发现之前的空白单元格已经有数字进来了：  </p><p><img src="http://pah1qyen2.bkt.clouddn.com/blog/180620/L39DBI9i4c.png?imageslim" alt="mark">  </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><code>VBA</code>还是要多查<code>MSDN</code>  </li><li><code>ADO</code>里的<code>Field</code>的基本属性要熟练</li></ul>]]></content>
      
      <categories>
          
          <category> VBA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ADO </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用scrapy爬取职位信息</title>
      <link href="/2018/06/15/%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF/"/>
      <url>/2018/06/15/%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF/</url>
      <content type="html"><![CDATA[<p>​    </p><a id="more"></a><p>新开一篇，来讲解一个<code>51JOB</code>工作职位的爬取。<br>目标：<br>爬取<code>51JOB</code>上所有与化工相关的职位信息。  </p><p>工具：<br><code>scrapy</code>框架+<code>Python3</code>  </p><h2 id="第一步：安装环境"><a href="#第一步：安装环境" class="headerlink" title="第一步：安装环境"></a>第一步：安装环境</h2><p>使用：  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtualenv -<span class="selector-tag">p</span> /usr/bin/python3 scrapy_env</span><br></pre></td></tr></table></figure><p>来创建一个新的虚拟环境来运行<code>scrapy</code>  </p><p>cd到<code>scrapy_env</code>文件夹，然后执行下面语句来开启虚拟空间：  </p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/scrapy_env$ source bin/<span class="built_in">activate</span></span><br></pre></td></tr></table></figure><p>进入到<code>scrapy_env</code>的虚拟空间后，用<code>pip3 install scrapy</code>来安装<code>scrapy</code>所需要的依赖  </p><h2 id="创建Scrapy项目"><a href="#创建Scrapy项目" class="headerlink" title="创建Scrapy项目"></a>创建<code>Scrapy</code>项目</h2><p>使用<code>scrapy startproject</code>来创建项目。   </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy startproject 51job</span><br><span class="line">Error: Project names must <span class="keyword">begin</span> <span class="keyword">with</span> a letter <span class="keyword">and</span> contain <span class="keyword">only</span></span><br><span class="line">letters, numbers <span class="keyword">and</span> underscores</span><br><span class="line">~$ scrapy startproject jobs</span><br><span class="line"><span class="keyword">New</span> Scrapy <span class="keyword">project</span> <span class="string">'jobs'</span>, <span class="keyword">using</span> <span class="keyword">template</span> <span class="keyword">directory</span> <span class="string">'/home/ubuntu/scrapy_env/lib/python3.5/site-packages/scrapy/templates/project'</span>, created <span class="keyword">in</span>:</span><br><span class="line">    /home/ubuntu/jobs</span><br><span class="line"></span><br><span class="line">You can <span class="keyword">start</span> your <span class="keyword">first</span> spider <span class="keyword">with</span>:</span><br><span class="line">    cd jobs</span><br><span class="line">    scrapy genspider example example.com</span><br></pre></td></tr></table></figure><p>然后使用<code>scrapy genspider</code>来创建一个爬虫  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~/jobs$ scrapy genspider JobSpider search.<span class="number">51</span>job.com</span><br><span class="line">Created spider <span class="string">'JobSpider'</span> using template <span class="string">'basic'</span> <span class="keyword">in</span> module:</span><br><span class="line">  jobs<span class="selector-class">.spiders</span><span class="selector-class">.JobSpider</span></span><br></pre></td></tr></table></figure><p>这样，项目的框架就搭好了。现在整个项目的目录文件应该是这样子的：  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── jobs</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── middlewares.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   ├── __init__<span class="selector-class">.cpython-35</span><span class="selector-class">.pyc</span></span><br><span class="line">│   │   └── settings<span class="selector-class">.cpython-35</span><span class="selector-class">.pyc</span></span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── JobSpider.py</span><br><span class="line">│       └── __pycache__</span><br><span class="line">└── scrapy.cfg</span><br></pre></td></tr></table></figure><h2 id="编辑Scrapy文件"><a href="#编辑Scrapy文件" class="headerlink" title="编辑Scrapy文件"></a>编辑<code>Scrapy</code>文件</h2><p><code>Scrapy</code>框架极大程度的减少了我们爬虫额代码编写量。对于一个简单的爬虫，我们只需要修改下面几个内容。 </p><p>首次得打开网页链接：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http<span class="variable">s:</span>//<span class="built_in">search</span>.<span class="number">51</span>job.<span class="keyword">com</span>/<span class="keyword">list</span>/<span class="number">000000</span>,<span class="number">000000</span>,<span class="number">0000</span>,<span class="number">19</span>,<span class="number">9</span>,<span class="number">99</span>,%<span class="number">2</span>B,<span class="number">2</span>,<span class="number">1</span>.html?lang=<span class="keyword">c</span>&amp;stype=<span class="number">1</span>&amp;postchannel=<span class="number">0000</span>&amp;workyear=<span class="number">99</span>&amp;cotype=<span class="number">99</span>&amp;degreefrom=<span class="number">99</span>&amp;jobterm=<span class="number">99</span>&amp;companysize=<span class="number">99</span>&amp;lonlat=<span class="number">0</span>%<span class="number">2</span>C0&amp;radius=-<span class="number">1</span>&amp;ord_field=<span class="number">0</span>&amp;confirmdate=<span class="number">9</span>&amp;fromType=<span class="number">1</span>&amp;dibiaoid=<span class="number">0</span>&amp;address=&amp;<span class="built_in">line</span>=&amp;specialarea=<span class="number">00</span>&amp;from=&amp;welfare=</span><br></pre></td></tr></table></figure></p><h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a><code>items.py</code></h3><p>我们需要获取的每一条招聘内容包括了下面这些信息：  </p><ul><li>职位名</li><li>公司名</li><li>工作地点</li><li>薪资</li><li>发布时间<br>因此，我们需要根据这些来定义一个<code>Item</code>，也就是一条爬取下来的信息。  </li></ul><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import <span class="keyword">scrapy</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">class </span><span class="keyword">JobsItem(scrapy.Item):</span></span><br><span class="line"><span class="keyword"> </span>   <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="keyword">job_title </span>= <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   company = <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   <span class="keyword">job_href </span>= <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   location = <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   salary = <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   post_date = <span class="keyword">scrapy.Field()</span></span><br></pre></td></tr></table></figure><p>这就好了，讲白了，我们是定义了一个名字叫<code>JobItem</code>的字典，然后给这个字典定义了5个键。  </p><h3 id="spiders-JobSpider-py"><a href="#spiders-JobSpider-py" class="headerlink" title="spiders/JobSpider.py"></a><code>spiders/JobSpider.py</code></h3><p>确定了我们要获取的信息，我们就可以处理爬虫了。  </p><p>爬取51job其实非常简单，我们只需要输入指定的关键字，然后点搜索，把返回的网页网址作为起始网址即可。  </p><p>然后要遵循下面的这个思路：</p><ul><li>爬某个页面</li><li>把我们所需要的字段信息装载到<code>item</code>里，后面我们要通过<code>pipeline</code>来进行处理</li><li>找到<code>下一页</code>的<code>anchor</code>标签的链接地址</li><li>如果能找到<code>下一页</code>的话，就递归回到第一步</li></ul><p>在解析页面的时候，需要用到<code>scrapy</code>的<code>selector</code>对象的<code>xpath</code>方法。<code>xpath</code>相对于<code>css</code>选择器来说更加复杂一些，但解析效率更高。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> jobs.items <span class="keyword">import</span> JobsItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobspiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name=<span class="string">'jobspider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'search.51job.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare='</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">        next_page_url = response.xpath(<span class="string">'//li[@class="bk"][2]/a/@href'</span>)</span><br><span class="line">        job_list = response.xpath(<span class="string">'//*[@id="resultList"]/div[@class="el"]'</span>)</span><br><span class="line">        <span class="keyword">for</span> each_job <span class="keyword">in</span> job_list:</span><br><span class="line">            job_info = JobsItem()</span><br><span class="line">            job_info[<span class="string">'job_title'</span>] = each_job.xpath(<span class="string">'.//p[contains(@class,"t1")]/span/a/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'company'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t2")]/a/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'job_href'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t2")]/a/@href'</span>)</span><br><span class="line">            job_info[<span class="string">'location'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t3")]/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'salary'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t4")]/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'post_date'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t5")]/text()'</span>) <span class="comment"># mm-dd</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> job_info.items():</span><br><span class="line">                <span class="keyword">if</span> v:</span><br><span class="line">                    job_info[k] = v.extract_first().strip()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    job_info[k] = <span class="string">'unknown'</span></span><br><span class="line">            <span class="keyword">yield</span> job_info</span><br><span class="line">        <span class="keyword">if</span> next_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            abs_url = next_page_url.extract_first().strip()</span><br><span class="line">            <span class="keyword">yield</span> response.follow(abs_url, callback=self.parse)</span><br></pre></td></tr></table></figure><h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a><code>pipelines.py</code></h3><p><code>pipelines</code>用来处理<code>scrapy</code>爬取完页面然后解析出来的<code>item</code>。可以这么理解，<code>pipelines</code>把<code>item</code>处理完之后，会装入到数据库里去。<br>在这里，我们要把数据装入到<code>MYSQL</code>数据库里，并且先不考虑去重等要求。  </p><blockquote><p>这需要在<code>MySQL</code>中先建立好<code>DATABASE</code>以及<code>TABLE</code>。</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import pymysql</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobsPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.database = pymysql.connect(host=<span class="string">'localhost'</span>,</span><br><span class="line">                                  port=<span class="number">3306</span>,</span><br><span class="line">                                  user=<span class="string">'xxxxxx'</span>,</span><br><span class="line">                                  passwd=<span class="string">'xxxxxx'</span>,</span><br><span class="line">                                  db=<span class="string">'job_info'</span>,</span><br><span class="line">                                  charset=<span class="string">'utf8'</span>)</span><br><span class="line">        <span class="keyword">self</span>.cursor = <span class="keyword">self</span>.database.cursor()</span><br><span class="line">        <span class="keyword">self</span>.table = <span class="string">'jobs'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line"></span><br><span class="line">        sql_string = <span class="string">'INSERT INTO &#123;&#125; (job_title, company, job_href, location, salary, post_date, update_datetime)  \</span></span><br><span class="line"><span class="string">                VALUES("&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;",str_to_date("&#123;&#125;","%Y-%m-%d %H:%i:%s"));'</span> \</span><br><span class="line">            .format(<span class="keyword">self</span>.table, item[<span class="string">'job_title'</span>], item[<span class="string">'company'</span>], item[<span class="string">'job_href'</span>],</span><br><span class="line">                    item[<span class="string">'location'</span>], item[<span class="string">'salary'</span>], item[<span class="string">'post_date'</span>],</span><br><span class="line">                    datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>))</span><br><span class="line">        <span class="symbol">try:</span></span><br><span class="line">            <span class="keyword">self</span>.cursor.execute(sql_string)</span><br><span class="line">            <span class="keyword">self</span>.database.commit()</span><br><span class="line">        <span class="symbol">except:</span></span><br><span class="line">            print(<span class="string">'error'</span>)</span><br><span class="line">            <span class="keyword">self</span>.database.rollback()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a><code>settings.py</code></h3><p>完成上面的处理之后，我们还需要做最后一步，修改<code>settings.py</code>，也就是项目设置。  </p><p>最主要的是两件事情：  </p><ul><li>设置默认的请求头<br>设置默认的请求头很简单，只需要找到，修改为下面的内容即可。  </li></ul><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span><br><span class="line">    'Accept-Language': 'en-US,en;q=0.9',</span><br><span class="line">    'Connection':'keep-alive',</span><br><span class="line">    'Cookie':'your-cookie-here',</span><br><span class="line">    'Host':'search.51job.com',</span><br><span class="line">    'Referer':'http://search.51job.com/list/<span class="number">000000</span>,<span class="number">000000</span>,<span class="number">0000</span>,19,9,99,%2B,2,2.html?lang=c&amp;stype=1&amp;postchannel=<span class="number">0000</span>&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=%27',</span><br><span class="line">    'Upgrade-Insecure-Requests':'1',</span><br><span class="line">    'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/65.0.<span class="number">3325.18</span>1 Chrome/65.0.<span class="number">3325.18</span>1 Safari/537.36',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那上面这些内容是从哪来的呢？当然是从<code>Chrome</code>浏览器的<code>Request Headers</code>的信息拿出来的。  </p><ul><li>开启<code>pipelines</code><br>这个也很简单，把下面这句取消注释即可。  <figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   'jobs.pipelines.JobsPipeline': 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="最终运行"><a href="#最终运行" class="headerlink" title="最终运行"></a>最终运行</h2><p>cd到项目的根目录下，注意一定要是根目录，然后执行：  </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">scrapy crawl jobspider</span></span><br></pre></td></tr></table></figure><p>至此，一个最简单的<code>Scrapy</code>爬虫就写好了。<br>后期还有很多细节要处理，比如数据去重，异常处理，数据分析等等，不过这些我们可以在后面优化，后面我们也会陆续讲解。</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Scrapy </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
