<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[用scrapy爬取职位信息]]></title>
    <url>%2F2018%2F06%2F15%2F%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[​ 新开一篇，来讲解一个51JOB工作职位的爬取。目标：爬取51JOB上所有与化工相关的职位信息。 工具：scrapy框架+Python3 第一步：安装环境使用： 1virtualenv -p /usr/bin/python3 scrapy_env 来创建一个新的虚拟环境来运行scrapy cd到scrapy_env文件夹，然后执行下面语句来开启虚拟空间： 1~/scrapy_env$ source bin/activate 进入到scrapy_env的虚拟空间后，用pip3 install scrapy来安装scrapy所需要的依赖 创建Scrapy项目使用scrapy startproject来创建项目。 12345678910$ scrapy startproject 51jobError: Project names must begin with a letter and contain onlyletters, numbers and underscores~$ scrapy startproject jobsNew Scrapy project 'jobs', using template directory '/home/ubuntu/scrapy_env/lib/python3.5/site-packages/scrapy/templates/project', created in: /home/ubuntu/jobsYou can start your first spider with: cd jobs scrapy genspider example example.com 然后使用scrapy genspider来创建一个爬虫 123~/jobs$ scrapy genspider JobSpider search.51job.comCreated spider 'JobSpider' using template 'basic' in module: jobs.spiders.JobSpider 这样，项目的框架就搭好了。现在整个项目的目录文件应该是这样子的： 123456789101112131415.├── jobs│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── __pycache__│ │ ├── __init__.cpython-35.pyc│ │ └── settings.cpython-35.pyc│ ├── settings.py│ └── spiders│ ├── __init__.py│ ├── JobSpider.py│ └── __pycache__└── scrapy.cfg 编辑Scrapy文件Scrapy框架极大程度的减少了我们爬虫额代码编写量。对于一个简单的爬虫，我们只需要修改下面几个内容。 首次得打开网页链接：1https://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare= items.py我们需要获取的每一条招聘内容包括了下面这些信息： 职位名 公司名 工作地点 薪资 发布时间因此，我们需要根据这些来定义一个Item，也就是一条爬取下来的信息。 1234567891011import scrapyclass JobsItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() job_title = scrapy.Field() company = scrapy.Field() job_href = scrapy.Field() location = scrapy.Field() salary = scrapy.Field() post_date = scrapy.Field() 这就好了，讲白了，我们是定义了一个名字叫JobItem的字典，然后给这个字典定义了5个键。 spiders/JobSpider.py确定了我们要获取的信息，我们就可以处理爬虫了。 爬取51job其实非常简单，我们只需要输入指定的关键字，然后点搜索，把返回的网页网址作为起始网址即可。 然后要遵循下面的这个思路： 爬某个页面 把我们所需要的字段信息装载到item里，后面我们要通过pipeline来进行处理 找到下一页的anchor标签的链接地址 如果能找到下一页的话，就递归回到第一步 在解析页面的时候，需要用到scrapy的selector对象的xpath方法。xpath相对于css选择器来说更加复杂一些，但解析效率更高。 1234567891011121314151617181920212223242526272829303132import scrapyfrom jobs.items import JobsItemclass JobspiderSpider(scrapy.Spider): name='jobspider' allowed_domains = ['search.51job.com'] start_urls = [ 'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare='] def parse(self, response): next_page_url = response.xpath('//li[@class="bk"][2]/a/@href') job_list = response.xpath('//*[@id="resultList"]/div[@class="el"]') for each_job in job_list: job_info = JobsItem() job_info['job_title'] = each_job.xpath('.//p[contains(@class,"t1")]/span/a/text()') job_info['company'] = each_job.xpath('.//span[contains(@class,"t2")]/a/text()') job_info['job_href'] = each_job.xpath('.//span[contains(@class,"t2")]/a/@href') job_info['location'] = each_job.xpath('.//span[contains(@class,"t3")]/text()') job_info['salary'] = each_job.xpath('.//span[contains(@class,"t4")]/text()') job_info['post_date'] = each_job.xpath('.//span[contains(@class,"t5")]/text()') # mm-dd for k, v in job_info.items(): if v: job_info[k] = v.extract_first().strip() else: job_info[k] = 'unknown' yield job_info if next_page_url is not None: abs_url = next_page_url.extract_first().strip() yield response.follow(abs_url, callback=self.parse) pipelines.pypipelines用来处理scrapy爬取完页面然后解析出来的item。可以这么理解，pipelines把item处理完之后，会装入到数据库里去。在这里，我们要把数据装入到MYSQL数据库里，并且先不考虑去重等要求。 这需要在MySQL中先建立好DATABASE以及TABLE。 123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-import pymysqlimport datetimeclass JobsPipeline(object): def open_spider(self, spider): self.database = pymysql.connect(host='localhost', port=3306, user='xxxxxx', passwd='xxxxxx', db='job_info', charset='utf8') self.cursor = self.database.cursor() self.table = 'jobs' def process_item(self, item, spider): sql_string = 'INSERT INTO &#123;&#125; (job_title, company, job_href, location, salary, post_date, update_datetime) \ VALUES("&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;",str_to_date("&#123;&#125;","%Y-%m-%d %H:%i:%s"));' \ .format(self.table, item['job_title'], item['company'], item['job_href'], item['location'], item['salary'], item['post_date'], datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')) try: self.cursor.execute(sql_string) self.database.commit() except: print('error') self.database.rollback() return item settings.py完成上面的处理之后，我们还需要做最后一步，修改settings.py，也就是项目设置。 最主要的是两件事情： 设置默认的请求头设置默认的请求头很简单，只需要找到，修改为下面的内容即可。 12345678910DEFAULT_REQUEST_HEADERS = &#123; 'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-US,en;q=0.9', 'Connection':'keep-alive', 'Cookie':'your-cookie-here', 'Host':'search.51job.com', 'Referer':'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,2.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=%27', 'Upgrade-Insecure-Requests':'1', 'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/65.0.3325.181 Chrome/65.0.3325.181 Safari/537.36',&#125; 那上面这些内容是从哪来的呢？当然是从Chrome浏览器的Request Headers的信息拿出来的。 开启pipelines这个也很简单，把下面这句取消注释即可。 123ITEM_PIPELINES = &#123; 'jobs.pipelines.JobsPipeline': 300,&#125; 最终运行cd到项目的根目录下，注意一定要是根目录，然后执行： 1scrapy crawl jobspider 至此，一个最简单的Scrapy爬虫就写好了。后期还有很多细节要处理，比如数据去重，异常处理，数据分析等等，不过这些我们可以在后面优化，后面我们也会陆续讲解。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
</search>
