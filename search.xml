<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>用scrapy爬取职位信息</title>
      <link href="/2018/06/15/%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF/"/>
      <url>/2018/06/15/%E7%94%A8scrapy%E7%88%AC%E5%8F%96%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF/</url>
      <content type="html"><![CDATA[<p>​    </p><a id="more"></a><p>新开一篇，来讲解一个<code>51JOB</code>工作职位的爬取。<br>目标：<br>爬取<code>51JOB</code>上所有与化工相关的职位信息。  </p><p>工具：<br><code>scrapy</code>框架+<code>Python3</code>  </p><h2 id="第一步：安装环境"><a href="#第一步：安装环境" class="headerlink" title="第一步：安装环境"></a>第一步：安装环境</h2><p>使用：  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtualenv -<span class="selector-tag">p</span> /usr/bin/python3 scrapy_env</span><br></pre></td></tr></table></figure><p>来创建一个新的虚拟环境来运行<code>scrapy</code>  </p><p>cd到<code>scrapy_env</code>文件夹，然后执行下面语句来开启虚拟空间：  </p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/scrapy_env$ source bin/<span class="built_in">activate</span></span><br></pre></td></tr></table></figure><p>进入到<code>scrapy_env</code>的虚拟空间后，用<code>pip3 install scrapy</code>来安装<code>scrapy</code>所需要的依赖  </p><h2 id="创建Scrapy项目"><a href="#创建Scrapy项目" class="headerlink" title="创建Scrapy项目"></a>创建<code>Scrapy</code>项目</h2><p>使用<code>scrapy startproject</code>来创建项目。   </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy startproject 51job</span><br><span class="line">Error: Project names must <span class="keyword">begin</span> <span class="keyword">with</span> a letter <span class="keyword">and</span> contain <span class="keyword">only</span></span><br><span class="line">letters, numbers <span class="keyword">and</span> underscores</span><br><span class="line">~$ scrapy startproject jobs</span><br><span class="line"><span class="keyword">New</span> Scrapy <span class="keyword">project</span> <span class="string">'jobs'</span>, <span class="keyword">using</span> <span class="keyword">template</span> <span class="keyword">directory</span> <span class="string">'/home/ubuntu/scrapy_env/lib/python3.5/site-packages/scrapy/templates/project'</span>, created <span class="keyword">in</span>:</span><br><span class="line">    /home/ubuntu/jobs</span><br><span class="line"></span><br><span class="line">You can <span class="keyword">start</span> your <span class="keyword">first</span> spider <span class="keyword">with</span>:</span><br><span class="line">    cd jobs</span><br><span class="line">    scrapy genspider example example.com</span><br></pre></td></tr></table></figure><p>然后使用<code>scrapy genspider</code>来创建一个爬虫  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~/jobs$ scrapy genspider JobSpider search.<span class="number">51</span>job.com</span><br><span class="line">Created spider <span class="string">'JobSpider'</span> using template <span class="string">'basic'</span> <span class="keyword">in</span> module:</span><br><span class="line">  jobs<span class="selector-class">.spiders</span><span class="selector-class">.JobSpider</span></span><br></pre></td></tr></table></figure><p>这样，项目的框架就搭好了。现在整个项目的目录文件应该是这样子的：  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── jobs</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── middlewares.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   ├── __init__<span class="selector-class">.cpython-35</span><span class="selector-class">.pyc</span></span><br><span class="line">│   │   └── settings<span class="selector-class">.cpython-35</span><span class="selector-class">.pyc</span></span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── JobSpider.py</span><br><span class="line">│       └── __pycache__</span><br><span class="line">└── scrapy.cfg</span><br></pre></td></tr></table></figure><h2 id="编辑Scrapy文件"><a href="#编辑Scrapy文件" class="headerlink" title="编辑Scrapy文件"></a>编辑<code>Scrapy</code>文件</h2><p><code>Scrapy</code>框架极大程度的减少了我们爬虫额代码编写量。对于一个简单的爬虫，我们只需要修改下面几个内容。 </p><p>首次得打开网页链接：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http<span class="variable">s:</span>//<span class="built_in">search</span>.<span class="number">51</span>job.<span class="keyword">com</span>/<span class="keyword">list</span>/<span class="number">000000</span>,<span class="number">000000</span>,<span class="number">0000</span>,<span class="number">19</span>,<span class="number">9</span>,<span class="number">99</span>,%<span class="number">2</span>B,<span class="number">2</span>,<span class="number">1</span>.html?lang=<span class="keyword">c</span>&amp;stype=<span class="number">1</span>&amp;postchannel=<span class="number">0000</span>&amp;workyear=<span class="number">99</span>&amp;cotype=<span class="number">99</span>&amp;degreefrom=<span class="number">99</span>&amp;jobterm=<span class="number">99</span>&amp;companysize=<span class="number">99</span>&amp;lonlat=<span class="number">0</span>%<span class="number">2</span>C0&amp;radius=-<span class="number">1</span>&amp;ord_field=<span class="number">0</span>&amp;confirmdate=<span class="number">9</span>&amp;fromType=<span class="number">1</span>&amp;dibiaoid=<span class="number">0</span>&amp;address=&amp;<span class="built_in">line</span>=&amp;specialarea=<span class="number">00</span>&amp;from=&amp;welfare=</span><br></pre></td></tr></table></figure></p><h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a><code>items.py</code></h3><p>我们需要获取的每一条招聘内容包括了下面这些信息：  </p><ul><li>职位名</li><li>公司名</li><li>工作地点</li><li>薪资</li><li>发布时间<br>因此，我们需要根据这些来定义一个<code>Item</code>，也就是一条爬取下来的信息。  </li></ul><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import <span class="keyword">scrapy</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">class </span><span class="keyword">JobsItem(scrapy.Item):</span></span><br><span class="line"><span class="keyword"> </span>   <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="keyword">job_title </span>= <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   company = <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   <span class="keyword">job_href </span>= <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   location = <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   salary = <span class="keyword">scrapy.Field()</span></span><br><span class="line"><span class="keyword"> </span>   post_date = <span class="keyword">scrapy.Field()</span></span><br></pre></td></tr></table></figure><p>这就好了，讲白了，我们是定义了一个名字叫<code>JobItem</code>的字典，然后给这个字典定义了5个键。  </p><h3 id="spiders-JobSpider-py"><a href="#spiders-JobSpider-py" class="headerlink" title="spiders/JobSpider.py"></a><code>spiders/JobSpider.py</code></h3><p>确定了我们要获取的信息，我们就可以处理爬虫了。  </p><p>爬取51job其实非常简单，我们只需要输入指定的关键字，然后点搜索，把返回的网页网址作为起始网址即可。  </p><p>然后要遵循下面的这个思路：</p><ul><li>爬某个页面</li><li>把我们所需要的字段信息装载到<code>item</code>里，后面我们要通过<code>pipeline</code>来进行处理</li><li>找到<code>下一页</code>的<code>anchor</code>标签的链接地址</li><li>如果能找到<code>下一页</code>的话，就递归回到第一步</li></ul><p>在解析页面的时候，需要用到<code>scrapy</code>的<code>selector</code>对象的<code>xpath</code>方法。<code>xpath</code>相对于<code>css</code>选择器来说更加复杂一些，但解析效率更高。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> jobs.items <span class="keyword">import</span> JobsItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobspiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name=<span class="string">'jobspider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'search.51job.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://search.51job.com/list/000000,000000,0000,19,9,99,%2B,2,1.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare='</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">        next_page_url = response.xpath(<span class="string">'//li[@class="bk"][2]/a/@href'</span>)</span><br><span class="line">        job_list = response.xpath(<span class="string">'//*[@id="resultList"]/div[@class="el"]'</span>)</span><br><span class="line">        <span class="keyword">for</span> each_job <span class="keyword">in</span> job_list:</span><br><span class="line">            job_info = JobsItem()</span><br><span class="line">            job_info[<span class="string">'job_title'</span>] = each_job.xpath(<span class="string">'.//p[contains(@class,"t1")]/span/a/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'company'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t2")]/a/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'job_href'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t2")]/a/@href'</span>)</span><br><span class="line">            job_info[<span class="string">'location'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t3")]/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'salary'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t4")]/text()'</span>)</span><br><span class="line">            job_info[<span class="string">'post_date'</span>] = each_job.xpath(<span class="string">'.//span[contains(@class,"t5")]/text()'</span>) <span class="comment"># mm-dd</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> job_info.items():</span><br><span class="line">                <span class="keyword">if</span> v:</span><br><span class="line">                    job_info[k] = v.extract_first().strip()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    job_info[k] = <span class="string">'unknown'</span></span><br><span class="line">            <span class="keyword">yield</span> job_info</span><br><span class="line">        <span class="keyword">if</span> next_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            abs_url = next_page_url.extract_first().strip()</span><br><span class="line">            <span class="keyword">yield</span> response.follow(abs_url, callback=self.parse)</span><br></pre></td></tr></table></figure><h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a><code>pipelines.py</code></h3><p><code>pipelines</code>用来处理<code>scrapy</code>爬取完页面然后解析出来的<code>item</code>。可以这么理解，<code>pipelines</code>把<code>item</code>处理完之后，会装入到数据库里去。<br>在这里，我们要把数据装入到<code>MYSQL</code>数据库里，并且先不考虑去重等要求。  </p><blockquote><p>这需要在<code>MySQL</code>中先建立好<code>DATABASE</code>以及<code>TABLE</code>。</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import pymysql</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobsPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.database = pymysql.connect(host=<span class="string">'localhost'</span>,</span><br><span class="line">                                  port=<span class="number">3306</span>,</span><br><span class="line">                                  user=<span class="string">'xxxxxx'</span>,</span><br><span class="line">                                  passwd=<span class="string">'xxxxxx'</span>,</span><br><span class="line">                                  db=<span class="string">'job_info'</span>,</span><br><span class="line">                                  charset=<span class="string">'utf8'</span>)</span><br><span class="line">        <span class="keyword">self</span>.cursor = <span class="keyword">self</span>.database.cursor()</span><br><span class="line">        <span class="keyword">self</span>.table = <span class="string">'jobs'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line"></span><br><span class="line">        sql_string = <span class="string">'INSERT INTO &#123;&#125; (job_title, company, job_href, location, salary, post_date, update_datetime)  \</span></span><br><span class="line"><span class="string">                VALUES("&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;","&#123;&#125;",str_to_date("&#123;&#125;","%Y-%m-%d %H:%i:%s"));'</span> \</span><br><span class="line">            .format(<span class="keyword">self</span>.table, item[<span class="string">'job_title'</span>], item[<span class="string">'company'</span>], item[<span class="string">'job_href'</span>],</span><br><span class="line">                    item[<span class="string">'location'</span>], item[<span class="string">'salary'</span>], item[<span class="string">'post_date'</span>],</span><br><span class="line">                    datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>))</span><br><span class="line">        <span class="symbol">try:</span></span><br><span class="line">            <span class="keyword">self</span>.cursor.execute(sql_string)</span><br><span class="line">            <span class="keyword">self</span>.database.commit()</span><br><span class="line">        <span class="symbol">except:</span></span><br><span class="line">            print(<span class="string">'error'</span>)</span><br><span class="line">            <span class="keyword">self</span>.database.rollback()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a><code>settings.py</code></h3><p>完成上面的处理之后，我们还需要做最后一步，修改<code>settings.py</code>，也就是项目设置。  </p><p>最主要的是两件事情：  </p><ul><li>设置默认的请求头<br>设置默认的请求头很简单，只需要找到，修改为下面的内容即可。  </li></ul><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span><br><span class="line">    'Accept-Language': 'en-US,en;q=0.9',</span><br><span class="line">    'Connection':'keep-alive',</span><br><span class="line">    'Cookie':'your-cookie-here',</span><br><span class="line">    'Host':'search.51job.com',</span><br><span class="line">    'Referer':'http://search.51job.com/list/<span class="number">000000</span>,<span class="number">000000</span>,<span class="number">0000</span>,19,9,99,%2B,2,2.html?lang=c&amp;stype=1&amp;postchannel=<span class="number">0000</span>&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=1&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=%27',</span><br><span class="line">    'Upgrade-Insecure-Requests':'1',</span><br><span class="line">    'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/65.0.<span class="number">3325.18</span>1 Chrome/65.0.<span class="number">3325.18</span>1 Safari/537.36',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那上面这些内容是从哪来的呢？当然是从<code>Chrome</code>浏览器的<code>Request Headers</code>的信息拿出来的。  </p><ul><li>开启<code>pipelines</code><br>这个也很简单，把下面这句取消注释即可。  <figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   'jobs.pipelines.JobsPipeline': 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="最终运行"><a href="#最终运行" class="headerlink" title="最终运行"></a>最终运行</h2><p>cd到项目的根目录下，注意一定要是根目录，然后执行：  </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">scrapy crawl jobspider</span></span><br></pre></td></tr></table></figure><p>至此，一个最简单的<code>Scrapy</code>爬虫就写好了。<br>后期还有很多细节要处理，比如数据去重，异常处理，数据分析等等，不过这些我们可以在后面优化，后面我们也会陆续讲解。</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Scrapy </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
